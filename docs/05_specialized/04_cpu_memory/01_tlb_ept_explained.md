---
level: specialized
estimated_time: 90 min
prerequisites:
  - 02_intermediate/03_complete_virtualization/03_hardware_optimizations.md
next_recommended:
  - 05_specialized/04_cpu_memory/02_tlb_capacity_limits.md
tags: [virtualization, tlb, ept, npt, vpid, memory, performance]
---

# TLB and Page Walk Caching with EPT/NPT

## The Core Question

With two-level translation (guest page tables + EPT), how does the TLB cache work? Does it get flushed constantly?

**Short answer:** The TLB caches the **final result** (GVA â†’ HPA), and with **VPID tagging**, no flush is needed on VM switches!

---

## Understanding the Translation Chain

### Without Virtualization (Normal)

```
Virtual Address â†’ Physical Address

Example:
  VA: 0x401000
    â†“ TLB lookup
  PA: 0x8234000
  
TLB caches: VA 0x401000 â†’ PA 0x8234000

Simple, one-level translation
```

---

### With EPT/NPT (Nested)

```
Guest Virtual Address (GVA)
    â†“ Guest Page Tables (4 levels)
Guest Physical Address (GPA)
    â†“ EPT/NPT (4 levels)
Host Physical Address (HPA)

Example:
  GVA: 0x401000
    â†“ Walk guest PT (4 levels)
  GPA: 0x10234000
    â†“ Walk EPT (4 levels)
  HPA: 0x8234000
  
Question: What does TLB cache?
```

---

## What the TLB Actually Caches

**The TLB caches the FINAL translation: GVA â†’ HPA**

```
TLB Entry:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Virtual Address: 0x401000               â”‚
â”‚ Physical Address: 0x8234000 (HPA!)      â”‚
â”‚ Permissions: R/W/X                      â”‚
â”‚ Page Size: 4KB                          â”‚
â”‚ Tags: VPID, ASID, etc.                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NOT cached: The intermediate GPA!

Why? Because what matters for memory access
is the final HPA. GPA is just an intermediate step.
```

---

## The Translation Process (Detailed)

### On TLB Miss

```
CPU needs to access GVA 0x401000:

Step 1: Check TLB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TLB lookup for 0x401000
  â†’ Miss! Not in cache

Step 2: Hardware Page Walk (MMU does this automatically)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Walk Guest Page Tables:
  1. Read guest CR3 â†’ guest PML4 base (GPA)
     
  2. GPA of PML4 table â†’ Need to translate to HPA!
     â†’ Walk EPT to get HPA of guest PML4
     â†’ Read guest PML4 entry â†’ Get guest PDPT GPA
     
  3. GPA of PDPT â†’ Need to translate to HPA!
     â†’ Walk EPT to get HPA of guest PDPT
     â†’ Read guest PDPT entry â†’ Get guest PD GPA
     
  4. GPA of PD â†’ Need to translate to HPA!
     â†’ Walk EPT to get HPA of guest PD
     â†’ Read guest PD entry â†’ Get guest PT GPA
     
  5. GPA of PT â†’ Need to translate to HPA!
     â†’ Walk EPT to get HPA of guest PT
     â†’ Read guest PT entry â†’ Get GPA of actual page!
     
  6. GPA of data page (0x10234000) â†’ Translate to HPA!
     â†’ Walk EPT one more time
     â†’ Get final HPA: 0x8234000

Worst case: 4 guest levels Ã— 5 memory accesses each (EPT walk)
           = 20-24 memory accesses!

Step 3: Cache in TLB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TLB entry created:
  GVA 0x401000 â†’ HPA 0x8234000
  
Next access to 0x401000:
  â†’ TLB hit! Direct access
  â†’ No page walk needed
  â†’ 1 cycle instead of 24 memory accesses
```

---

## The Problem: TLB Invalidation

### Without VPID - Constant Flushing

```
Scenario: Two VMs running

VM1 TLB entries:
  GVA 0x1000 â†’ HPA 0x5000 (VM1's memory)
  GVA 0x2000 â†’ HPA 0x6000
  
VM2 TLB entries:
  GVA 0x1000 â†’ HPA 0x9000 (VM2's memory)
  GVA 0x2000 â†’ HPA 0xA000

Problem: Same GVA, different HPA!

Without tagging:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. VM1 running:
   - TLB has: 0x1000 â†’ 0x5000
   
2. VM Exit (switch to VM2):
   - Must flush entire TLB!
   - Otherwise VM2 might use VM1's translations
   - Security issue: VM2 accessing VM1's memory!
   
3. VM2 running:
   - TLB empty (just flushed)
   - Every memory access = TLB miss
   - Expensive page walks
   
4. VM Exit (switch back to VM1):
   - Must flush entire TLB again!
   
Result: Constant TLB flushes
        Performance killer!
```

**Performance impact:**

```
Benchmark: VM switches (1000 times)

Without VPID:
  - Every switch: TLB flush
  - First 100 instructions after switch: All TLB misses
  - 100 Ã— 24 memory accesses = 2400 memory accesses
  - 2400 Ã— 100ns = 240Î¼s overhead per switch
  - 1000 switches = 240ms overhead
  
With warm TLB (no flush):
  - Most accesses: TLB hit
  - Negligible overhead
  
TLB flush = 20-30% performance loss!
```

---

## The Solution: VPID (Virtual Processor ID)

### VPID Tagging

```
Extended TLB Entry Format:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VPID: 1                                 â”‚ â† NEW!
â”‚ Virtual Address: 0x1000                 â”‚
â”‚ Physical Address: 0x5000                â”‚
â”‚ Permissions: R/W/X                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VPID: 2                                 â”‚ â† Different VPID!
â”‚ Virtual Address: 0x1000                 â”‚ â† Same VA!
â”‚ Physical Address: 0x9000                â”‚ â† Different PA!
â”‚ Permissions: R/W/X                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Both entries can coexist in TLB!
```

---

### How VPID Works

```
Setup:
â”€â”€â”€â”€â”€â”€
Hypervisor assigns VPIDs:
  VM1 â†’ VPID 1
  VM2 â†’ VPID 2
  Host â†’ VPID 0

Each VMCS contains VPID field:
  VM1's VMCS: VPID = 1
  VM2's VMCS: VPID = 2

CPU tracks current VPID in hardware register
```

**VM Switch with VPID:**

```
1. VM1 running (VPID=1):
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TLB lookups filter by VPID=1:
     Access 0x1000:
       â†’ Check TLB for (VPID=1, VA=0x1000)
       â†’ Hit: 0x5000
       â†’ Use 0x5000
   
2. VM Exit â†’ VM2 Entry (switch to VPID=2):
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   VMRESUME instruction:
     - Load VPID=2 from VMCS
     - Update CPU's current VPID register
     - NO TLB FLUSH!
   
3. VM2 running (VPID=2):
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TLB lookups filter by VPID=2:
     Access 0x1000:
       â†’ Check TLB for (VPID=2, VA=0x1000)
       â†’ Hit: 0x9000
       â†’ Use 0x9000
   
   TLB entry (VPID=1, VA=0x1000) still exists!
   But filtered out because current VPID != 1
   
4. VM Exit â†’ VM1 Entry (switch back to VPID=1):
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   VMRESUME:
     - Load VPID=1
     - NO TLB FLUSH!
   
   TLB still has (VPID=1, VA=0x1000) â†’ 0x5000
   Immediate TLB hit!
```

---

### VPID Benefits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario              â”‚ Without    â”‚ With VPID   â”‚
â”‚                       â”‚ VPID       â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VM switch overhead    â”‚ ~500 cyclesâ”‚ ~200 cycles â”‚
â”‚ TLB flush?            â”‚ Yes        â”‚ No          â”‚
â”‚ TLB misses after      â”‚ 100%       â”‚ ~5%         â”‚
â”‚ switch                â”‚            â”‚             â”‚
â”‚ Performance impact    â”‚ 20-30%     â”‚ 2-5%        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VPID enables:
âœ“ Different VMs can have overlapping GVA ranges
âœ“ TLB stays warm across VM switches
âœ“ Hypervisor can have its own VPID (host mode)
âœ“ Massive performance improvement (15-30%)
```

---

## Additional Page Walk Caches

**TLB is not the only cache!**

### The Page Walk Cache Hierarchy

```
1. TLB (Translation Lookaside Buffer)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Caches: Complete translations (GVA â†’ HPA)
   Size: ~64-1024 entries per core
   Tagged: VPID, PCID, Global bit
   
2. Page Walk Cache (PWC) / Paging Structure Cache
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Caches: Intermediate page table entries
   
   For guest page tables:
     GVA of PML4 entry â†’ HPA of PML4 entry
     GVA of PDPT entry â†’ HPA of PDPT entry
     GVA of PD entry â†’ HPA of PD entry
     GVA of PT entry â†’ HPA of PT entry
   
   Size: ~16-32 entries per level per core
   
3. EPT Page Walk Cache
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Caches: EPT intermediate translations
   
   For EPT:
     GPA of EPT PML4 â†’ HPA of EPT PML4
     GPA of EPT PDPT â†’ HPA of EPT PDPT
     ...
   
   Size: ~16-32 entries per level per core
```

---

### Why Multiple Caches Matter

**Scenario: TLB miss but PWC hit**

```
CPU accesses GVA 0x401000:

Step 1: Check TLB
  â†’ Miss!
  
Step 2: Start page walk
  Need guest PML4 entry:
    Check PWC: (guest CR3, offset 0) â†’ HPA of PML4 entry
    â†’ Hit! Skip guest PML4 EPT walk (saved 5 memory accesses)
    
  Need guest PDPT entry:
    Check PWC: (PML4 GPA, offset 8) â†’ HPA of PDPT entry  
    â†’ Hit! Skip EPT walk (saved 5 more memory accesses)
    
  Need guest PD entry:
    Check PWC: (PDPT GPA, offset 16) â†’ HPA of PD entry
    â†’ Hit! Skip EPT walk (saved 5 more memory accesses)
    
  Need guest PT entry:
    Check PWC: (PD GPA, offset 24) â†’ HPA of PT entry
    â†’ Hit! Skip EPT walk (saved 5 more memory accesses)
    
  Need final page:
    Must walk EPT (no cache)
    â†’ 5 memory accesses

Total: 5 memory accesses (vs 24 without PWC)

Update TLB with final translation
```

---

### Cache Invalidation

**When are these caches flushed?**

```
TLB flush triggers:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. CR3 write (without PCID)
   â†’ Flush all non-global TLB entries
   
2. INVLPG instruction
   â†’ Flush specific page
   
3. INVPCID instruction
   â†’ Selective invalidation by PCID/VPID
   
4. VM entry/exit (without VPID)
   â†’ Full flush
   
5. EPT modifications
   â†’ INVEPT instruction flushes EPT caches

With VPID:
  VM switches DON'T flush TLB!
  Only explicit invalidation instructions

Page Walk Cache flush:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Typically flushed together with TLB
But sometimes preserved (implementation-specific)

EPT Page Walk Cache flush:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INVEPT instruction:
  - INVEPT type 1: Single-context invalidation
  - INVEPT type 2: All-context invalidation
```

---

## Real-World Example: Context Switch

### Without VPID

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€
T=0: VM1 running, TLB warm
     Access 0x1000 â†’ TLB hit â†’ 1 cycle
     
T=1: VM Exit (interrupt)
     - Save VM1 state
     - Flush TLB (security!)
     - Load host state
     
T=2: Host (KVM) running, TLB cold
     Access 0x401000 â†’ TLB miss â†’ Page walk â†’ 24 accesses
     Access 0x402000 â†’ TLB miss â†’ Page walk â†’ 24 accesses
     ...
     TLB gradually warms up
     
T=3: VM Entry (VM2)
     - Flush TLB (security!)
     - Load VM2 state
     
T=4: VM2 running, TLB cold
     Access 0x1000 â†’ TLB miss â†’ Page walk â†’ 24 accesses
     Access 0x2000 â†’ TLB miss â†’ Page walk â†’ 24 accesses
     ...
     TLB gradually warms up
     
T=5: VM Exit
     - Flush TLB
     
T=6: VM Entry (VM1)
     - Flush TLB
     
T=7: VM1 running, TLB cold AGAIN
     Access 0x1000 â†’ TLB miss â†’ Page walk â†’ 24 accesses
     
Every VM switch: Cold TLB, massive overhead
```

---

### With VPID

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€
T=0: VM1 (VPID=1) running, TLB warm
     Access 0x1000 â†’ TLB hit (VPID=1) â†’ 1 cycle
     
T=1: VM Exit (interrupt)
     - Save VM1 state
     - NO TLB FLUSH!
     - Switch to VPID=0 (host)
     
T=2: Host (VPID=0) running
     Access 0x401000:
       â†’ Check TLB (VPID=0, VA=0x401000)
       â†’ Miss (first time)
       â†’ Page walk, cache result
     
     Access 0x401000 again:
       â†’ TLB hit (VPID=0) â†’ 1 cycle
     
     VM1's entries still in TLB (VPID=1)!
     
T=3: VM Entry (VM2, VPID=2)
     - NO TLB FLUSH!
     - Switch to VPID=2
     
T=4: VM2 (VPID=2) running
     Access 0x1000:
       â†’ Check TLB (VPID=2, VA=0x1000)
       â†’ Miss (first time for VM2)
       â†’ Page walk, cache result
     
     Access 0x1000 again:
       â†’ TLB hit (VPID=2) â†’ 1 cycle
     
     VM1's and Host's entries still in TLB!
     
T=5: VM Exit, VM Entry (VM1, VPID=1)
     - NO TLB FLUSH!
     - Switch to VPID=1
     
T=6: VM1 (VPID=1) running
     Access 0x1000:
       â†’ Check TLB (VPID=1, VA=0x1000)
       â†’ HIT! Entry from T=0 still there!
       â†’ 1 cycle
     
TLB stays warm across switches!
All three contexts coexist in TLB!
```

---

## Performance Impact

**Benchmark: Rapid context switches**

```
Test: Switch between 4 VMs, 1000 switches each

Without VPID:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 850ms
Breakdown:
  - Context switch overhead: 100ms
  - TLB misses after switch: 750ms
  
TLB miss rate after switch: 95%

With VPID:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 180ms
Breakdown:
  - Context switch overhead: 100ms
  - TLB misses after switch: 80ms
  
TLB miss rate after switch: 8%

Speedup: 4.7x improvement!
```

---

## Modern Enhancements

### PCID (Process-Context Identifier)

**Similar to VPID, but for processes within guest:**

```
Without PCID:
  Process switch within guest â†’ CR3 write â†’ TLB flush
  
With PCID:
  TLB entries tagged by PCID
  Multiple processes' TLB entries coexist
  No flush on process switch
  
Combined with VPID:
  TLB entry tagged: (VPID=1, PCID=5, VA=0x1000)
  Allows:
    - Multiple VMs
    - Multiple processes per VM
    - All with warm TLBs!
```

---

### Huge Pages

**Reduce TLB pressure:**

```
4KB pages:
  TLB entry covers: 4KB
  1GB memory: Needs 262,144 TLB entries
  TLB size: ~64-1024 entries
  TLB miss rate: High
  
2MB huge pages:
  TLB entry covers: 2MB
  1GB memory: Needs 512 TLB entries
  TLB miss rate: Much lower
  
1GB huge pages:
  TLB entry covers: 1GB
  1GB memory: Needs 1 TLB entry!
  TLB miss rate: Minimal
  
With EPT huge pages:
  Guest can use huge pages
  AND EPT can map with huge pages
  Double benefit!
```

---

## Summary

**Does TLB get flushed with EPT/NPT?**

**Without VPID: YES**
- Every VM switch â†’ TLB flush
- 20-30% performance penalty
- TLB constantly cold

**With VPID: NO**
- TLB entries tagged by VPID
- Different VMs' entries coexist
- TLB stays warm
- 15-30% performance improvement

**What does TLB cache?**
- Final translation: GVA â†’ HPA
- NOT the intermediate GPA
- Tagged with VPID (and PCID)

**Additional caching:**
- Page Walk Cache: Intermediate page table entries
- EPT Page Walk Cache: EPT intermediate entries
- Both reduce page walk overhead

**The innovation:** VPID allows multiple virtual address spaces (different VMs) to have TLB entries simultaneously, eliminating the need for flushes on context switches. This is critical for virtualization performance!

---

## Hands-On Resources

> ðŸ’¡ **Want more?** This section shows the most essential resources for this topic.
> For a comprehensive list of tutorials, code repositories, and tools across all virtualization topics, see:
> **â†’ [Complete Virtualization Learning Resources](../../../01_foundations/00_VIRTUALIZATION_RESOURCES.md)** ðŸ“š

**Focused resources for TLB, EPT, and memory virtualization:**

- **[Intel SDM on EPT and TLB](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)** - Chapters 28-29: EPT and TLB management in virtualization
- **[KVM MMU Documentation](https://www.kernel.org/doc/html/latest/virt/kvm/mmu.html)** - Linux KVM memory management unit implementation details
