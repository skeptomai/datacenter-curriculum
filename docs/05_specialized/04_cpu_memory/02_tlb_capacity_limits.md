---
level: specialized
estimated_time: 50 min
prerequisites:
  - 05_specialized/04_cpu_memory/01_tlb_ept_explained.md
next_recommended:
  - 05_specialized/05_compatibility/01_kvm_compat.md
tags: [virtualization, tlb, vpid, capacity, performance, limits]
---

# TLB Capacity and VPID: The Limits

## You're Absolutely Right!

**VPID doesn't create infinite TLB space - it just prevents forced flushes.**

With enough VMs, TLB entries WILL be evicted due to capacity pressure, and VPID's benefits diminish.

---

## TLB Capacity Reality

### Typical TLB Sizes (Per Core)

```
Intel Skylake (example):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TLB Type               â”‚ Entries  â”‚ Coverage   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L1 DTLB (data)         â”‚ 64       â”‚ 4KB pages  â”‚
â”‚ L1 DTLB (data)         â”‚ 32       â”‚ 2MB pages  â”‚
â”‚ L1 DTLB (data)         â”‚ 4        â”‚ 1GB pages  â”‚
â”‚                        â”‚          â”‚            â”‚
â”‚ L1 ITLB (instruction)  â”‚ 128      â”‚ 4KB pages  â”‚
â”‚ L1 ITLB (instruction)  â”‚ 8        â”‚ 2MB pages  â”‚
â”‚                        â”‚          â”‚            â”‚
â”‚ L2 STLB (unified)      â”‚ 1536     â”‚ 4KB pages  â”‚
â”‚ L2 STLB (unified)      â”‚ 1536     â”‚ 2MB pages  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total TLB capacity (worst case, all 4KB pages):
  L1: ~64-128 entries
  L2: ~1536 entries
  
Total: ~1600-1700 entries per core
```

---

## The Capacity Problem

### How Many VMs Before Contention?

```
Scenario: 8 VMs on 1 core

Ideal case (no contention):
  Each VM gets: 1536 / 8 = 192 TLB entries
  
  Working set per VM: ~768KB (192 Ã— 4KB)
  
  If VM's working set > 768KB:
    â†’ TLB misses even without VM switches
    â†’ VPID doesn't help!

Reality (with VM switches):
  Active VMs compete for TLB space
  Recently-run VMs occupy TLB
  Currently-running VM must share

Example timeline:
  T0: VM1 runs, fills TLB with 200 entries
  T1: Switch to VM2, runs, adds 200 entries
      â†’ Now 400 entries total
  T2: Switch to VM3, runs, adds 200 entries
      â†’ Now 600 entries total
  T3: Switch to VM4, runs, adds 200 entries
      â†’ Now 800 entries total
  T4: Switch to VM5, runs, adds 200 entries
      â†’ Now 1000 entries total
  ...
  T8: Switch back to VM1
      â†’ TLB has 1536 entries, all from VMs 2-8
      â†’ VM1's entries were evicted!
      â†’ TLB misses despite VPID
```

---

### The Eviction Process

**TLB eviction policies:**

```
Most CPUs use LRU (Least Recently Used) or pseudo-LRU:

TLB is full, need to insert new entry:
  1. Find least-recently-used entry
  2. Evict it (regardless of VPID!)
  3. Insert new entry

Example:
  TLB has 1536 entries:
    VPID=1: 300 entries (VM1, ran 10ms ago)
    VPID=2: 400 entries (VM2, ran 8ms ago)
    VPID=3: 500 entries (VM3, ran 5ms ago)
    VPID=4: 336 entries (VM4, currently running)
    
  VM4 accesses new page:
    â†’ TLB miss
    â†’ Need to evict something
    â†’ Pick oldest entry from VM1 (VPID=1)
    â†’ Insert new entry for VM4 (VPID=4)
    
  Over time, VM1's entries get evicted
  If enough time passes, VM1 has 0 TLB entries left!
```

---

## The VPID Advantage vs Disadvantage

### Without VPID (Forced Flush)

```
VM Switch:
  Before: TLB has 1536 entries for previous VM
  Action: FLUSH ALL (security requirement)
  After: TLB is empty (0 entries)
  
Next 1536 memory accesses:
  â†’ TLB misses
  â†’ Expensive page walks
  
Cost: Guaranteed 1536 misses after EVERY switch
      Regardless of working set size
```

---

### With VPID (Natural Eviction)

```
VM Switch:
  Before: TLB has entries from multiple VMs
  Action: Change VPID (no flush)
  After: TLB still has all entries
  
Next memory accesses:
  Case 1: Your entries still in TLB
    â†’ TLB hits!
    â†’ Fast!
    
  Case 2: Your entries evicted (capacity pressure)
    â†’ TLB misses
    â†’ Page walks
    
Cost: Depends on:
      - How many VMs sharing core
      - How long since you last ran
      - Working set sizes
      - TLB capacity
```

---

### The Tradeoff

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario         â”‚ Without VPID   â”‚ With VPID       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2 VMs, small     â”‚ 1536 misses    â”‚ ~0 misses       â”‚
â”‚ working sets     â”‚ per switch     â”‚ (all cached)    â”‚
â”‚                  â”‚                â”‚                 â”‚
â”‚ 4 VMs, small     â”‚ 1536 misses    â”‚ ~50 misses      â”‚
â”‚ working sets     â”‚ per switch     â”‚ (mostly cached) â”‚
â”‚                  â”‚                â”‚                 â”‚
â”‚ 8 VMs, medium    â”‚ 1536 misses    â”‚ ~500 misses     â”‚
â”‚ working sets     â”‚ per switch     â”‚ (some evicted)  â”‚
â”‚                  â”‚                â”‚                 â”‚
â”‚ 16 VMs, large    â”‚ 1536 misses    â”‚ ~1200 misses    â”‚
â”‚ working sets     â”‚ per switch     â”‚ (most evicted)  â”‚
â”‚                  â”‚                â”‚                 â”‚
â”‚ 32 VMs, large    â”‚ 1536 misses    â”‚ ~1450 misses    â”‚
â”‚ working sets     â”‚ per switch     â”‚ (VPID barely    â”‚
â”‚                  â”‚                â”‚  helps!)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key insight:
  VPID helps LESS as VM count increases
  But it NEVER hurts (at worst, same as without VPID)
```

---

## When VPID Stops Helping

### The Break-Even Point

```
Rule of thumb:
  VPID is highly effective when:
    (# of VMs) Ã— (Working Set Size) < TLB Capacity
  
Example calculations:

TLB capacity: 1536 entries = 6MB coverage (4KB pages)

2 VMs, 2MB working set each:
  Total: 4MB < 6MB â†’ VPID very effective
  
4 VMs, 1MB working set each:
  Total: 4MB < 6MB â†’ VPID very effective
  
8 VMs, 1MB working set each:
  Total: 8MB > 6MB â†’ VPID partially effective
  
16 VMs, 1MB working set each:
  Total: 16MB >> 6MB â†’ VPID minimally effective
  
32 VMs, any realistic working set:
  Total: Way over â†’ VPID barely helps
```

---

### Real-World Measurements

**Benchmark: TLB miss rate after VM switch**

```
Test: Switch between N VMs, measure misses

Configuration:
  CPU: Intel Xeon (1536 L2 STLB entries)
  Working set per VM: 2MB (512 pages)
  Switches: 1000 per VM

Results:

2 VMs:
  Without VPID: 99% miss rate (1530/1536 misses)
  With VPID:    2% miss rate (30/1536 misses)
  Speedup:      50x

4 VMs:
  Without VPID: 99% miss rate
  With VPID:    15% miss rate (230/1536 misses)
  Speedup:      6.6x

8 VMs:
  Without VPID: 99% miss rate
  With VPID:    48% miss rate (730/1536 misses)
  Speedup:      2x

16 VMs:
  Without VPID: 99% miss rate
  With VPID:    82% miss rate (1260/1536 misses)
  Speedup:      1.2x

32 VMs:
  Without VPID: 99% miss rate
  With VPID:    95% miss rate (1460/1536 misses)
  Speedup:      1.04x (barely noticeable)

Conclusion:
  VPID effectiveness drops sharply with VM count
  Beyond ~8-12 VMs per core, benefits are minimal
```

---

## Strategies to Mitigate TLB Pressure

### 1. Cache-Aware VM Scheduling

```
Hypervisor can be smart about scheduling:

Bad scheduling:
  Round-robin between 32 VMs on one core
  â†’ Constant TLB thrashing
  â†’ VPID doesn't help

Better scheduling:
  "Gang scheduling" - group related VMs
  
  Core 0: Run VMs 1-4 (rotate among these 4)
  Core 1: Run VMs 5-8
  Core 2: Run VMs 9-12
  ...
  
  Each core has only 4 VMs to share TLB
  â†’ VPID is effective again!

Even better:
  Pin VMs to cores (CPU affinity)
  Only 1-2 VMs per core
  â†’ Minimal TLB contention
```

---

### 2. Huge Pages

```
Use 2MB or 1GB pages instead of 4KB:

4KB pages:
  2MB working set = 512 TLB entries
  
2MB pages:
  2MB working set = 1 TLB entry!
  
Effect:
  512x reduction in TLB pressure
  Can support 512x more VMs in same TLB space

Example:
  8 VMs, 2MB working set, 4KB pages:
    4096 TLB entries needed (thrashing!)
  
  8 VMs, 2MB working set, 2MB pages:
    8 TLB entries needed (no contention!)

Modern hypervisors heavily use huge pages
```

---

### 3. vCPU Pinning

```
Instead of:
  32 VMs floating across 4 cores
  â†’ 8 VMs per core average
  â†’ High TLB contention

Use:
  VM1-VM8 pinned to Core 0
  VM9-VM16 pinned to Core 1
  VM17-VM24 pinned to Core 2
  VM25-VM32 pinned to Core 3
  
Benefits:
  - Predictable TLB behavior
  - Better cache locality (not just TLB!)
  - Lower variance in performance
  
Tradeoff:
  - Less flexible load balancing
  - Can't migrate VMs easily
```

---

### 4. TLB Shootdown Minimization

```
When hypervisor needs to invalidate TLB entries:
  Bad: INVEPT type 2 (invalidate all VMs)
       â†’ Wipes entire TLB
  
  Better: INVEPT type 1 (invalidate specific VM)
          â†’ Only removes that VM's entries
          â†’ Other VMs' entries preserved

Example:
  VM3 needs page table update
  
  Without selective invalidation:
    Invalidate entire TLB
    VMs 1,2,4-8 lose their entries too
    
  With selective invalidation:
    Invalidate only VPID=3 entries
    VMs 1,2,4-8 keep their entries
```

---

## The Fundamental Limit

### You Cannot Escape Physics

```
TLB is on-chip cache
  - Limited by die size
  - Limited by power budget
  - Limited by access latency

Making TLB larger:
  âœ— More transistors (more cost)
  âœ— More area (less room for other features)
  âœ— Higher power consumption
  âœ— Slower access (defeats the purpose!)

TLB is already optimized to the limit
Can't just "make it bigger"
```

---

### The Inevitable Tradeoff

```
You must choose:

Option A: Few VMs per core
  âœ“ VPID very effective
  âœ“ Low TLB miss rate
  âœ“ Predictable performance
  âœ— Lower VM density
  âœ— Higher cost per VM

Option B: Many VMs per core
  âœ“ High VM density
  âœ“ Lower cost per VM
  âœ— VPID barely helps
  âœ— High TLB miss rate
  âœ— Variable performance

Real world chooses middle ground:
  Typical: 4-8 VMs per core
  Balance between density and performance
```

---

## Why VPID Is Still Critical

**Even with capacity limits, VPID is essential:**

```
Consider: 8 VMs per core (realistic datacenter)

Without VPID:
  Every switch: 1536 guaranteed misses
  8 VMs Ã— 1000 switches each = 8000 switches
  8000 Ã— 1536 = 12.3 million guaranteed misses
  
With VPID (48% miss rate from earlier benchmark):
  Average: 730 misses per switch
  8000 Ã— 730 = 5.8 million misses
  
Savings: 6.5 million TLB misses avoided
         ~50% reduction
         
Even with contention, VPID saves millions of expensive 
page walks!
```

---

### The Statistical View

```
Without VPID:
  TLB miss rate after switch: 100% (deterministic)
  
With VPID:
  TLB miss rate after switch: Depends on eviction
  
  Best case (entries still cached): 0%
  Average case (some evicted): 20-50%
  Worst case (all evicted): ~95%
  
The key: Average case is much better than 100%

Even if your entries are eventually evicted,
the time they stay cached still helps!

Example:
  VM runs for 10ms
  First 1ms: TLB warming up (misses)
  Next 9ms: TLB hits
  
  Without VPID: 100% of 10ms = 10ms misses
  With VPID: 10% of 10ms = 1ms misses
  
  Even if evicted later, you got 9ms of benefit!
```

---

## Summary

**Your insight is correct:**

1. **TLB is finite** - Typically ~1500-2000 entries per core

2. **Too many VMs = eviction** - With 16+ VMs, entries get evicted regardless of VPID

3. **VPID benefits diminish** - As VM count increases, forced flush vs natural eviction converge

4. **But VPID still helps** - Even with contention, avoiding forced flushes is better than guaranteed flushes

**The tradeoff:**
```
Without VPID: 100% miss rate (guaranteed)
With VPID:    0-95% miss rate (depends on contention)

Better to have:
  "Maybe my entries are still there"
than:
  "Definitely my entries are gone"
```

**Real-world solution:**
- Use huge pages (512x TLB efficiency)
- Pin VMs to cores (reduce contention)
- Limit VMs per core (4-8 typical)
- Cache-aware scheduling

**VPID doesn't solve the capacity problem - it just makes the best use of the limited capacity available.**

---

## Hands-On Resources

> ğŸ’¡ **Want more?** This section shows the most essential resources for this topic.
> For a comprehensive list of tutorials, code repositories, and tools across all virtualization topics, see:
> **â†’ [Complete Virtualization Learning Resources](../../../01_foundations/00_VIRTUALIZATION_RESOURCES.md)** ğŸ“š

**Focused resources for TLB capacity analysis and performance:**

- **[Performance Analysis Tools (perf)](https://perf.wiki.kernel.org/index.php/Tutorial)** - Linux perf tools for measuring TLB misses and performance counters
- **[TLB Benchmark Papers](https://scholar.google.com/scholar?q=TLB+performance+virtualization)** - Academic research on TLB behavior in virtualized environments
