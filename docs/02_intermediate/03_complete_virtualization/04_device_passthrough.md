---
level: intermediate
estimated_time: 50 min
prerequisites:
  - 02_intermediate/03_complete_virtualization/01_evolution_complete.md
  - 02_intermediate/03_complete_virtualization/03_hardware_optimizations.md
next_recommended:
  - 05_specialized/04_cpu_memory/01_tlb_ept_explained.md
  - 05_specialized/03_serverless/01_firecracker_relationship.md
tags: [virtualization, sr-iov, vfio, iommu, passthrough, performance]
---

# SR-IOV, Device Passthrough, and IOMMU Deep Dive

## Part 1: The Problem - Direct Device Access Without Isolation

### Why Can't We Just Give Devices to VMs?

**The fundamental security issue:**

```
Without protection:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VM wants to access disk
  â†“
Device uses DMA (Direct Memory Access)
  â†“
Device can read/write ANY physical memory!
  â†“
VM could program device to access:
  - Other VMs' memory
  - Hypervisor memory
  - Host kernel memory
  
DISASTER! Complete security breach!
```

**The DMA problem:**

```
Traditional DMA (no virtualization):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Device driver (in kernel):
  buffer = malloc(4096);
  physical_addr = virt_to_phys(buffer);
  
  Tell device: "DMA to physical address 0x12345000"
  
Device:
  Receives command: DMA to 0x12345000
  Places data directly at that physical address
  No CPU involvement
  
This works because:
  - Only kernel can program device
  - Kernel is trusted
  - Single address space

With VMs:
â”€â”€â”€â”€â”€â”€â”€â”€â”€

VM1 driver thinks it has physical address 0x12345000
  But that's Guest Physical Address (GPA)
  Real host physical address (HPA) is 0x98765000
  
If device DMAs to 0x12345000 (what VM told it):
  â†’ Wrong memory!
  â†’ Might be VM2's memory
  â†’ Might be hypervisor memory
  
Security disaster!
```

---

## Part 2: IOMMU - The Solution

### What is IOMMU?

**IOMMU = I/O Memory Management Unit**

```
Intel calls it: VT-d (Virtualization Technology for Directed I/O)
AMD calls it: AMD-Vi (AMD I/O Virtualization)

Just like MMU translates virtual addresses for CPU:
  CPU MMU: Virtual Address â†’ Physical Address
  
IOMMU translates DMA addresses for devices:
  IOMMU: Device Virtual Address (IOVA) â†’ Host Physical Address (HPA)
```

---

### IOMMU Architecture

```
Without IOMMU:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CPU   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Memory Bus              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â†’ Memory (RAM)
           â”‚
           â””â”€â”€â”€â”€â”€â”€â†’ PCIe Bus
                        â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ NIC     â”‚ â† Device can DMA anywhere!
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Device issues DMA:
  "Write to address 0x12345000"
  â†’ Goes directly to that physical address
  â†’ No translation, no protection


With IOMMU:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CPU   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   IOMMU     â”‚ â† Intercepts device DMA!
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Memory Bus                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â†’ Memory (RAM)
           â”‚
           â””â”€â”€â”€â”€â”€â”€â†’ PCIe Bus
                        â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ NIC     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Device issues DMA:
  "Write to IOVA 0x12345000"
  â†’ IOMMU intercepts
  â†’ Looks up in page table for THIS device
  â†’ Translates to HPA 0x98765000
  â†’ Writes to correct physical address
  â†’ Protection enforced!
```

---

### IOMMU Page Tables

**Per-device address spaces:**

```
Each device (or group) has its own IOMMU page table:

VM1's NIC:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IOMMU Page Table for NIC-VM1        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ IOVA         â”‚ HPA                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0x00000000   â”‚ 0x80000000 (VM1 RAM) â”‚
â”‚ 0x00001000   â”‚ 0x80001000           â”‚
â”‚ 0x00002000   â”‚ 0x80002000           â”‚
â”‚ ...          â”‚ ...                  â”‚
â”‚ 0x10000000   â”‚ Invalid (protect!)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Device can only DMA to addresses in its table
Cannot access VM2 or hypervisor memory!


VM2's NIC:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IOMMU Page Table for NIC-VM2        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ IOVA         â”‚ HPA                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0x00000000   â”‚ 0x90000000 (VM2 RAM) â”‚
â”‚ 0x00001000   â”‚ 0x90001000           â”‚
â”‚ ...          â”‚ ...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Completely isolated!
Each device sees only its VM's memory
```

---

### IOMMU Translation Process

```
Device performs DMA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. NIC-VM1 wants to DMA packet to address 0x12345000 (IOVA)

2. NIC issues memory write on PCIe bus:
   Write: data=<packet>, address=0x12345000

3. IOMMU intercepts (sitting on memory bus):
   - Identifies source device (PCIe bus:dev:func)
   - Looks up which page table to use
   - Finds: "NIC-VM1 uses page table at 0xABCD0000"

4. IOMMU walks page table (similar to CPU page walk):
   - Read page table base: 0xABCD0000
   - Extract indices from IOVA 0x12345000
   - PML4 index, PDPT index, PD index, PT index
   - Walk 4 levels
   - Find final entry: IOVA 0x12345000 â†’ HPA 0x80345000

5. IOMMU checks permissions:
   - Is mapping valid? Yes
   - Is write allowed? Yes
   - Is device allowed? Yes

6. IOMMU issues real memory write:
   Write: data=<packet>, address=0x80345000 (HPA)

7. Memory controller writes to RAM at 0x80345000

8. Done! VM1 receives packet in correct memory location

Time: ~100-200 ns overhead (vs no IOMMU)
But provides complete isolation!
```

---

## Part 3: Device Passthrough (VFIO)

### What is VFIO?

**VFIO = Virtual Function I/O**

```
VFIO is the Linux framework for safely passing devices to VMs

Key components:
  1. IOMMU driver (enables translation)
  2. VFIO driver (manages device assignment)
  3. User space API (QEMU/KVM uses this)
  4. Device groups (isolation boundaries)
```

---

### Passthrough Setup

**Step-by-step device assignment:**

```
Step 1: Enable IOMMU in BIOS/UEFI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Intel: Enable "VT-d"
AMD: Enable "AMD-Vi" or "IOMMU"


Step 2: Enable IOMMU in kernel
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Boot parameter: intel_iommu=on (Intel)
                amd_iommu=on (AMD)

Kernel loads:
  - iommu driver
  - vfio-pci driver


Step 3: Identify device
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
lspci -nn | grep Ethernet

Output:
  01:00.0 Ethernet controller [0200]: Intel Corporation 82599ES [8086:10fb]
  
PCIe address: 01:00.0 (bus:device.function)
Vendor:Device: 8086:10fb


Step 4: Unbind from host driver
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "0000:01:00.0" > /sys/bus/pci/drivers/ixgbe/unbind

Device no longer usable by host


Step 5: Bind to VFIO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "8086 10fb" > /sys/bus/pci/drivers/vfio-pci/new_id
echo "0000:01:00.0" > /sys/bus/pci/drivers/vfio-pci/bind

Device now managed by VFIO


Step 6: Configure IOMMU page tables
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
(Done by VFIO driver automatically)

Create IOMMU context for this device
Set up page tables mapping GPA â†’ HPA
Map VM's memory into IOMMU tables


Step 7: Pass to VM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
qemu-system-x86_64 \
  -device vfio-pci,host=01:00.0 \
  ...

VM boots, sees real NIC as PCIe device
Loads native driver (ixgbe in this case)
Full speed, direct access!
```

---

### What the Guest Sees

```
Inside the VM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

lspci

Output:
  00:05.0 Ethernet controller: Intel Corporation 82599ES
  
Guest sees a REAL PCIe device!
Not emulated, not paravirtualized
Actual hardware

Guest loads native driver:
  modprobe ixgbe
  
Driver works normally:
  - Programs device registers
  - Sets up DMA rings
  - Enables interrupts
  - Full hardware features

From guest perspective:
  It's bare metal!
  No difference from physical server
```

---

### DMA Flow with Passthrough

```
Guest driver sends packet:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Guest driver (ixgbe):
   skb = alloc_skb(1500);
   // Fill with packet data
   
   guest_virt_addr = skb->data;       // 0xffff888012345000
   guest_phys_addr = virt_to_phys();  // 0x12345000 (GPA)
   
   // Program NIC descriptor:
   tx_desc->buffer_addr = 0x12345000;  // GPA!
   tx_desc->length = 1500;
   
   // Kick NIC (write to tail register)
   writel(tail_idx, NIC_TDT_REG);

2. NIC reads descriptor:
   DMA read from descriptor ring
   IOMMU translates descriptor address
   Gets buffer_addr = 0x12345000 (IOVA from guest's perspective)

3. NIC DMAs packet data:
   DMA read from 0x12345000
   â†’ IOMMU intercepts
   â†’ Looks up in VM1-NIC page table
   â†’ Translates: 0x12345000 (GPA) â†’ 0x80345000 (HPA)
   â†’ Real DMA from 0x80345000
   
4. NIC sends packet on wire

5. NIC writes completion:
   DMA write to completion ring
   IOMMU translates
   Guest receives completion

No hypervisor involvement!
No VM exits for DMA!
```

---

## Part 4: SR-IOV (Single Root I/O Virtualization)

### The Problem with Basic Passthrough

```
Without SR-IOV:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

One physical NIC = One VM

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host   â”‚
â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”  â”‚
â”‚  â”‚VM1â”‚  â”‚ â† Gets entire NIC (passthrough)
â”‚  â””â”€â”€â”€â”˜  â”‚
â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”  â”‚
â”‚  â”‚VM2â”‚  â”‚ â† Needs another physical NIC!
â”‚  â””â”€â”€â”€â”˜  â”‚
â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problem:
  - Need one NIC per VM
  - Expensive
  - Uses PCIe slots
  - Doesn't scale

Can't share one NIC between VMs
  (Each needs dedicated device)
```

---

### SR-IOV Solution

**Single physical NIC â†’ Many virtual NICs**

```
SR-IOV NIC:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

One physical device presents multiple PCIe functions:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Physical NIC (Intel X710)            â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Physical Function (PF)             â”‚ â”‚ â† Full device
â”‚  â”‚ - Configuration                    â”‚ â”‚   Host manages
â”‚  â”‚ - Manages VFs                      â”‚ â”‚
â”‚  â”‚ - Has all features                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  VF 0   â”‚ â”‚  VF 1   â”‚ â”‚  VF 2   â”‚  â”‚ â† Virtual Functions
â”‚  â”‚ (Light) â”‚ â”‚ (Light) â”‚ â”‚ (Light) â”‚  â”‚   Pass to VMs
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       ...                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  VF 62  â”‚ â”‚  VF 63  â”‚               â”‚ â† Up to 64 VFs typical
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PCIe view:
  Bus 01:00.0 = PF (managed by host)
  Bus 01:00.1 = VF 0 (passed to VM1)
  Bus 01:00.2 = VF 1 (passed to VM2)
  Bus 01:00.3 = VF 2 (passed to VM3)
  ...
```

---

### How SR-IOV Works

**Hardware partitioning:**

```
Physical NIC resources:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TX Queues: 128 total
RX Queues: 128 total
MAC addresses: 64 (one per VF)
VLAN filters: 64
Interrupts: 64 MSI-X vectors

SR-IOV divides these:

PF (host):
  TX queues: 2
  RX queues: 2
  Management only
  
VF 0 (VM1):
  TX queues: 2
  RX queues: 2
  MAC: aa:bb:cc:dd:ee:00
  Own MSI-X vectors
  
VF 1 (VM2):
  TX queues: 2
  RX queues: 2
  MAC: aa:bb:cc:dd:ee:01
  Own MSI-X vectors
  
... (repeat for all VFs)

Each VF is isolated:
  - Separate queues
  - Separate interrupts
  - Separate MAC
  - Separate IOMMU mapping
```

---

### VF as Seen by Guest

```
Guest (VM1) perspective:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

lspci:
  00:05.0 Ethernet controller: Intel Corporation X710 Virtual Function

Looks like a real NIC!
But lighter weight (VF not PF)

Guest loads VF driver:
  modprobe iavf (Intel Adaptive Virtual Function driver)
  
Driver sees:
  - 2 TX queues
  - 2 RX queues  
  - MAC address
  - Can send/receive
  - Full speed DMA

Guest doesn't know:
  - It's a VF not PF
  - Other VFs exist
  - Sharing physical hardware

Isolation via IOMMU:
  VF0 can only DMA to VM1's memory
  VF1 can only DMA to VM2's memory
  Complete separation
```

---

### SR-IOV Packet Flow

```
VM1 sends packet via VF0:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Guest driver (iavf in VM1):
   skb = alloc_skb(1500);
   guest_phys = virt_to_phys(skb->data);  // 0x12345000 (GPA)
   
   // Program VF0 TX descriptor
   vf0_tx_desc[0].addr = 0x12345000;
   vf0_tx_desc[0].len = 1500;
   
   // Kick VF0 queue
   writel(1, VF0_TX_TAIL);

2. NIC hardware (VF0 context):
   - Read descriptor from VF0 TX queue
   - See buffer address: 0x12345000 (IOVA)
   - DMA read packet data
   
3. IOMMU for VF0:
   - IOVA 0x12345000 â†’ HPA 0x80345000 (VM1's memory)
   - DMA from 0x80345000
   
4. NIC hardware:
   - Get packet data
   - Add Ethernet header (src MAC = VF0's MAC)
   - Send on wire
   
5. NIC writes completion to VF0 TX completion queue
   - IOMMU translates
   - VM1 gets completion


VM2 receives packet via VF1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Packet arrives at physical NIC

2. NIC hardware:
   - Look at destination MAC
   - Match to VF1's MAC address
   - Route to VF1 RX queue

3. NIC DMAs packet to VF1 RX buffer:
   - VF1 RX descriptor has buffer address (IOVA)
   - IOMMU for VF1 translates
   - IOVA 0x22000000 â†’ HPA 0x90100000 (VM2's memory)
   - DMA write to 0x90100000

4. NIC sends MSI-X interrupt to VF1
   - Injected into VM2
   
5. VM2 guest driver processes packet:
   - Sees packet in RX buffer
   - Processes normally

Isolation:
  VF0 can't access VF1's buffers
  Different IOMMU page tables
  Hardware enforced
```

---

## Part 5: Performance Comparison

### Benchmark: 10 Gbps Network

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method             â”‚ Throughputâ”‚ Latency  â”‚ CPU Usage â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Emulated e1000     â”‚ 2 Gbps   â”‚ 100 Î¼s   â”‚ 80%       â”‚
â”‚ (full emulation)   â”‚          â”‚          â”‚           â”‚
â”‚                    â”‚          â”‚          â”‚           â”‚
â”‚ virtio-net         â”‚ 9.5 Gbps â”‚ 15 Î¼s    â”‚ 20%       â”‚
â”‚ (paravirt)         â”‚          â”‚          â”‚           â”‚
â”‚                    â”‚          â”‚          â”‚           â”‚
â”‚ vhost-net          â”‚ 9.8 Gbps â”‚ 10 Î¼s    â”‚ 10%       â”‚
â”‚ (kernel virtio)    â”‚          â”‚          â”‚           â”‚
â”‚                    â”‚          â”‚          â”‚           â”‚
â”‚ SR-IOV VF          â”‚ 9.95 Gbpsâ”‚ 5 Î¼s     â”‚ 2%        â”‚
â”‚ (passthrough)      â”‚          â”‚          â”‚           â”‚
â”‚                    â”‚          â”‚          â”‚           â”‚
â”‚ Bare metal         â”‚ 10 Gbps  â”‚ 4 Î¼s     â”‚ 1%        â”‚
â”‚ (no virtualization)â”‚          â”‚          â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SR-IOV is 98-99% of bare metal!
```

---

### Why SR-IOV is Fastest

```
virtio-net:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Guest â†’ virtqueue â†’ VM exit â†’ KVM â†’ vhost â†’ NIC
  Overhead:
    - virtqueue management
    - VM exit (even with vhost)
    - Shared memory coordination
    - Batching helps but not perfect

SR-IOV:
â”€â”€â”€â”€â”€â”€â”€
Guest â†’ VF â†’ NIC (direct!)
  Overhead:
    - IOMMU translation (~100 ns)
    - That's it!
  
No VM exits for data path!
No hypervisor involvement!
Direct hardware access!
```

---

### IOMMU Performance Impact

```
Without IOMMU (unsafe):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Device DMA: 50 ns
  Straight to memory
  
With IOMMU:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Device DMA: 150 ns
  - Device issues DMA (10 ns)
  - IOMMU intercepts (20 ns)
  - Page table walk (80 ns, can be cached)
  - Memory access (40 ns)
  
3x slower than no protection

BUT:
  - Still faster than virtio (no VM exits)
  - Caching helps (IOMMU TLB)
  - Modern IOMMUs are fast
  - Security worth the cost

Real world: <5% performance impact
```

---

## Part 6: IOMMU Groups and Isolation

### What is an IOMMU Group?

**Isolation boundary:**

```
Problem: PCIe topology matters

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CPU     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PCIe Root     â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
   â”‚          â”‚
   â”‚     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚     â”‚PCIe Bridgeâ”‚
   â”‚     â””â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
   â†“       â†“     â†“
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”
â”‚ NIC1 â”‚ â”‚NIC2â”‚â”‚NIC3â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜

IOMMU groups:
  Group 0: NIC1 (isolated)
  Group 1: Bridge + NIC2 + NIC3 (shared)

Why?
  NIC2 and NIC3 share a bridge
  Bridge can forward transactions
  NIC2 could potentially access NIC3's DMA
  Must be in same group!

If passing NIC2 to VM:
  - Must pass entire group
  - NIC3 also assigned
  - Or leave both in host
```

---

### Viewing IOMMU Groups

```bash
#!/bin/bash
# Show IOMMU groups

for d in /sys/kernel/iommu_groups/*/devices/*; do
    n=${d#*/iommu_groups/*}
    n=${n%%/*}
    printf 'IOMMU Group %s ' "$n"
    lspci -nns "${d##*/}"
done

Output:
â”€â”€â”€â”€â”€â”€
IOMMU Group 0: 00:00.0 Host bridge [0600]: Intel
IOMMU Group 1: 00:01.0 PCI bridge [0604]: Intel
IOMMU Group 2: 01:00.0 Ethernet [0200]: Intel 82599ES [8086:10fb]
IOMMU Group 2: 01:00.1 Ethernet [0200]: Intel 82599ES [8086:10fb]
IOMMU Group 3: 02:00.0 NVMe [0108]: Samsung 970 EVO

Group 2 has 2 ports: Must assign both or neither
```

---

## Part 7: SR-IOV with RDMA

**Perfect combination for storage:**

```
RoCE + SR-IOV:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Physical NIC: Mellanox ConnectX-5 (100 Gbps)
  - Supports RDMA (RoCE)
  - Supports SR-IOV (64 VFs)
  
Create VFs:
  echo 64 > /sys/class/net/eth0/device/sriov_numvfs
  
Assign to VMs:
  VM1: VF0 (100 Gbps RDMA)
  VM2: VF1 (100 Gbps RDMA)
  VM3: VF2 (100 Gbps RDMA)
  ...

Each VM gets:
  - Full RDMA capabilities
  - 100 Gbps bandwidth
  - <5 Î¼s latency
  - Zero-copy DMA
  - Direct to hardware
  
Perfect for:
  - NVMe-oF storage servers
  - Distributed databases
  - High-performance computing
  
Near bare-metal performance!
```

---

## Part 8: Tradeoffs and Limitations

### Advantages of SR-IOV

```
âœ“ Near-native performance (98%+)
âœ“ Low latency (<5 Î¼s)
âœ“ Low CPU overhead (2%)
âœ“ Hardware offloads work (checksums, TSO, RSS)
âœ“ No hypervisor in data path
âœ“ Scales to many VMs per NIC
```

---

### Disadvantages of SR-IOV

```
âœ— Limited VFs (typically 64 max)
  Can't do 1000 VMs with one NIC

âœ— No live migration
  Can't move VM to another host (device tied to hardware)
  Workaround: Bond with virtio, switch on migration

âœ— Hardware dependent
  Need SR-IOV capable NIC
  Need IOMMU support
  Guest needs VF driver

âœ— Less flexible
  Can't inspect traffic in hypervisor
  Can't apply policies easily
  Can't do QoS in software

âœ— Management complexity
  Must configure VFs
  Must handle IOMMU groups
  Must update VF firmware

âœ— No VF-to-VF optimization
  Two VMs on same host with VFs
  Traffic must go through physical switch
  (virtio can stay in host memory)
```

---

### When to Use What?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Use Case            â”‚ Recommended  â”‚ Why            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ General VMs         â”‚ virtio-net   â”‚ Good enough,   â”‚
â”‚ (web, app servers)  â”‚              â”‚ more flexible  â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ High-performance    â”‚ SR-IOV       â”‚ Need <10 Î¼s    â”‚
â”‚ storage (NVMe-oF)   â”‚              â”‚ latency        â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ RDMA applications   â”‚ SR-IOV       â”‚ Must be direct â”‚
â”‚                     â”‚              â”‚ hardware       â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ Network functions   â”‚ virtio-net   â”‚ Need traffic   â”‚
â”‚ (firewalls, LBs)    â”‚              â”‚ inspection     â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ High VM density     â”‚ virtio-net   â”‚ Limited VFs    â”‚
â”‚ (100s per host)     â”‚              â”‚                â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ Cloud public VMs    â”‚ virtio-net   â”‚ Live migration â”‚
â”‚                     â”‚              â”‚ required       â”‚
â”‚                     â”‚              â”‚                â”‚
â”‚ AI training         â”‚ SR-IOV       â”‚ Need max GPU   â”‚
â”‚ (GPU passthrough)   â”‚              â”‚ performance    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 9: Complete Example

### Setting Up SR-IOV for Storage VM

```bash
# 1. Check IOMMU support
dmesg | grep -i iommu

Output: DMAR: IOMMU enabled

# 2. List network devices
lspci | grep Ethernet

Output:
  01:00.0 Ethernet controller: Intel Corporation X710 (rev 02)
  01:00.1 Ethernet controller: Intel Corporation X710 (rev 02)

# 3. Enable SR-IOV on device
echo 8 > /sys/class/net/eth0/device/sriov_numvfs

# 4. Verify VFs created
lspci | grep Virtual

Output:
  01:02.0 Ethernet controller: Intel Corporation X710 Virtual Function
  01:02.1 Ethernet controller: Intel Corporation X710 Virtual Function
  01:02.2 Ethernet controller: Intel Corporation X710 Virtual Function
  ...

# 5. Unbind VF from host
echo "0000:01:02.0" > /sys/bus/pci/drivers/iavf/unbind

# 6. Bind to VFIO
echo "8086 154c" > /sys/bus/pci/drivers/vfio-pci/new_id
echo "0000:01:02.0" > /sys/bus/pci/drivers/vfio-pci/bind

# 7. Check IOMMU group
ls -l /sys/kernel/iommu_groups/*/devices/* | grep 01:02.0

Output: /sys/kernel/iommu_groups/10/devices/0000:01:02.0

# 8. Start VM with VF
qemu-system-x86_64 \
  -enable-kvm \
  -m 16G \
  -smp 8 \
  -device vfio-pci,host=01:02.0 \
  -drive file=storage.qcow2 \
  ...

# 9. In guest VM:
lspci
Output: 00:04.0 Ethernet controller: Intel Corporation X710 Virtual Function

ip link
Output: eth0: <BROADCAST,MULTICAST,UP> mtu 1500

# 10. Configure for RDMA/RoCE
modprobe rdma_rxe
rdma link add rxe0 type rxe netdev eth0

# 11. Test RDMA
ib_write_bw -d rxe0

Output: 9800 Mbps (near line rate!)

# Success! VM has direct RDMA access via SR-IOV VF
```

---

## Summary: The Complete Picture

```
Device Access Hierarchy:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Slowest â†’ Fastest:

1. Full Emulation (e1000)
   Guest â†’ VM exit â†’ QEMU emulates registers â†’ NIC
   Speed: 20% of native
   
2. Paravirtualization (virtio-net)
   Guest â†’ virtqueue â†’ VM exit â†’ KVM â†’ vhost â†’ NIC
   Speed: 95% of native
   
3. Passthrough (VFIO)
   Guest â†’ VF â†’ IOMMU â†’ NIC (direct)
   Speed: 98-99% of native

IOMMU provides:
  âœ“ Isolation (per-device page tables)
  âœ“ Translation (GPA â†’ HPA)
  âœ“ Protection (device can't access arbitrary memory)
  
SR-IOV provides:
  âœ“ Multiple virtual devices from one physical
  âœ“ Hardware partitioning (queues, MACs, interrupts)
  âœ“ Each VF isolated via IOMMU
  
Together:
  Near-native performance + Security + Scalability
  
Perfect for:
  - Storage (NVMe-oF, Ceph with RDMA)
  - AI/ML (GPU passthrough)
  - Network intensive workloads
  - Low-latency applications
```

**SR-IOV + IOMMU = How cloud providers achieve bare-metal performance for specialized workloads!**

---

## Hands-On Resources

> ğŸ’¡ **Want more?** This section shows the most essential resources for this topic.
> For a comprehensive list of tutorials, code repositories, and tools across all virtualization topics, see:
> **â†’ [Complete Virtualization Learning Resources](../../01_foundations/00_VIRTUALIZATION_RESOURCES.md)** ğŸ“š

**Focused resources for device passthrough, SR-IOV, and IOMMU:**

- **[VFIO Kernel Documentation](https://www.kernel.org/doc/Documentation/vfio.txt)** - Complete guide to VFIO (Virtual Function I/O) for device assignment
- **[SR-IOV Specification (PCI-SIG)](https://pcisig.com/specifications/iov/single_root/)** - Official PCI-SIG specification for Single Root I/O Virtualization

---

## Next Steps

Continue exploring advanced virtualization topics or return to the main learning path.
